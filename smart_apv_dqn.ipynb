{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29146d1b",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "- Synthetic data + environment encapsulation\n",
    "- Baseline controller plus KPIs\n",
    "- Lightweight DQN agent and training loop\n",
    "- Evaluation utilities and investor-friendly plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and global configuration\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-colorblind\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "TIME_STEPS = 96  # 15-minute resolution\n",
    "DELTA_T_HOURS = 0.25\n",
    "\n",
    "@dataclass\n",
    "class APVConfig:\n",
    "    pv_kwp: float = 100.0\n",
    "    bess_kwh: float = 200.0\n",
    "    bess_kw: float = 100.0\n",
    "    bess_efficiency: float = 0.92\n",
    "    pump_kw: float = 30.0\n",
    "    num_pumps: int = 2\n",
    "    soil_target: float = 0.6\n",
    "    soil_capacity: float = 1.0\n",
    "    soil_min: float = 0.2\n",
    "    water_per_kw: float = 1.6  # m3 delivered per kWh\n",
    "\n",
    "@dataclass\n",
    "class TariffConfig:\n",
    "    import_price: float = 0.12  # $/kWh\n",
    "    export_price: float = 0.05  # $/kWh credit\n",
    "\n",
    "@dataclass\n",
    "class RewardWeights:\n",
    "    energy_cost: float = 1.0\n",
    "    soil_stress: float = 4.5\n",
    "    bess_cycle: float = 0.04\n",
    "    irrigation_shortfall: float = 18.0\n",
    "\n",
    "APV_CFG = APVConfig()\n",
    "TARIFF = TariffConfig()\n",
    "REWARD_W = RewardWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic weather, load, and irrigation profiles\n",
    "def generate_synthetic_day(seed: int, steps: int = TIME_STEPS) -> Dict[str, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    hours = np.arange(steps) * DELTA_T_HOURS\n",
    "    solar_core = np.clip(np.sin((hours - 6) / 12 * math.pi), 0.0, 1.0)\n",
    "    cloud_noise = rng.normal(0.0, 0.12, steps)\n",
    "    irradiance = np.clip(solar_core + cloud_noise, 0.0, 1.0)\n",
    "    pv_kw = irradiance * APV_CFG.pv_kwp * 0.9\n",
    "    temp_c = 24 + 8 * irradiance + rng.normal(0.0, 1.5, steps)\n",
    "    temp_norm = np.clip((temp_c - 18) / 20, 0.0, 1.0)\n",
    "    base_load = 18 + 4 * np.sin((hours - 3) / 8) + rng.normal(0.0, 1.5, steps)\n",
    "    evening_bump = 6 * np.exp(-((hours - 19) ** 2) / 18)\n",
    "    base_load_kw = np.clip(base_load + evening_bump, 12, 40)\n",
    "    irrigation_target_m3 = rng.uniform(480, 640)\n",
    "    return {\n",
    "        \"irradiance\": irradiance,\n",
    "        \"pv_kw\": pv_kw,\n",
    "        \"temperature_norm\": temp_norm,\n",
    "        \"base_load_kw\": base_load_kw,\n",
    "        \"irrigation_target_m3\": irrigation_target_m3,\n",
    "        \"hours\": hours,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrivoltaic environment with reward shaping\n",
    "class AgrivoltaicEnv:\n",
    "    def __init__(self, apv_cfg: APVConfig = APV_CFG, tariff: TariffConfig = TARIFF, reward_weights: RewardWeights = REWARD_W):\n",
    "        self.cfg = apv_cfg\n",
    "        self.tariff = tariff\n",
    "        self.reward_weights = reward_weights\n",
    "        self.delta_t = DELTA_T_HOURS\n",
    "        self.max_steps = TIME_STEPS\n",
    "        self.state_dim = 8\n",
    "        self.action_space = 5\n",
    "        self.day: Dict[str, np.ndarray] | None = None\n",
    "        self._step_log: List[Dict[str, float]] = []\n",
    "        self.episode_metrics: Dict[str, float] = {}\n",
    "        self.episode_trace: List[Dict[str, float]] = []\n",
    "\n",
    "    def reset(self, seed: int | None = None) -> np.ndarray:\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 10_000)\n",
    "        self.day = generate_synthetic_day(seed, self.max_steps)\n",
    "        self.step_idx = 0\n",
    "        self.soc = 0.5 * self.cfg.bess_kwh\n",
    "        self._last_soc = self.soc\n",
    "        self.soil = self.cfg.soil_target\n",
    "        self.remaining_irrigation = 1.0\n",
    "        self.daily_target = float(self.day[\"irrigation_target_m3\"])\n",
    "        self.cumulative_cost = 0.0\n",
    "        self.cumulative_stress_steps = 0\n",
    "        self.bess_throughput = 0.0\n",
    "        self._episode_reward = 0.0\n",
    "        self.episode_metrics = {}\n",
    "        self._step_log = []\n",
    "        self.episode_trace = []\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        idx = min(self.step_idx, self.max_steps - 1)\n",
    "        pv_norm = float(self.day[\"pv_kw\"][idx] / self.cfg.pv_kwp)\n",
    "        base_norm = float(self.day[\"base_load_kw\"][idx] / 40.0)\n",
    "        obs = np.array([\n",
    "            idx / self.max_steps,\n",
    "            self.soc / self.cfg.bess_kwh,\n",
    "            self.soil,\n",
    "            pv_norm,\n",
    "            base_norm,\n",
    "            float(self.day[\"irradiance\"][idx]),\n",
    "            float(self.day[\"temperature_norm\"][idx]),\n",
    "            self.remaining_irrigation,\n",
    "        ], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict[str, float]]:\n",
    "        assert self.day is not None, \"Call reset() before step().\"\n",
    "        pv_kw = float(self.day[\"pv_kw\"][self.step_idx])\n",
    "        base_kw = float(self.day[\"base_load_kw\"][self.step_idx])\n",
    "        irradiance = float(self.day[\"irradiance\"][self.step_idx])\n",
    "        temp_norm = float(self.day[\"temperature_norm\"][self.step_idx])\n",
    "\n",
    "        pump_kw, irrig_m3 = self._pump_dispatch(action)\n",
    "        irrig_fraction = self._update_soil(irrig_m3, irradiance)\n",
    "        residual_kw = base_kw + pump_kw - pv_kw\n",
    "        bess_kw = self._bess_dispatch(action, residual_kw)\n",
    "        net_grid_kw = residual_kw - bess_kw\n",
    "        step_cost = self._settle_cost(net_grid_kw)\n",
    "        soil_penalty = abs(self.soil - self.cfg.soil_target)\n",
    "        delta_soc = abs(self.soc - self._last_soc)\n",
    "        reward = -(self.reward_weights.energy_cost * step_cost +\n",
    "                   self.reward_weights.soil_stress * soil_penalty +\n",
    "                   self.reward_weights.bess_cycle * delta_soc)\n",
    "        self._episode_reward += reward\n",
    "        self.cumulative_cost += step_cost\n",
    "        self.bess_throughput += delta_soc\n",
    "        if self.soil < self.cfg.soil_target:\n",
    "            self.cumulative_stress_steps += 1\n",
    "        self._record_step(action, pump_kw, bess_kw, net_grid_kw, pv_kw, base_kw, temp_norm, irrig_fraction, reward)\n",
    "        self._last_soc = self.soc\n",
    "\n",
    "        self.step_idx += 1\n",
    "        done = self.step_idx >= self.max_steps\n",
    "        if done:\n",
    "            shortfall_penalty = self.reward_weights.irrigation_shortfall * self.remaining_irrigation\n",
    "            reward -= shortfall_penalty\n",
    "            self._episode_reward -= shortfall_penalty\n",
    "            self._finalize_metrics()\n",
    "            next_obs = np.zeros(self.state_dim, dtype=np.float32)\n",
    "        else:\n",
    "            next_obs = self._get_obs()\n",
    "\n",
    "        info = {\n",
    "            \"net_grid_kw\": net_grid_kw,\n",
    "            \"step_cost\": step_cost,\n",
    "            \"soil\": self.soil,\n",
    "            \"soc\": self.soc / self.cfg.bess_kwh,\n",
    "            \"remaining_irrigation\": self.remaining_irrigation,\n",
    "        }\n",
    "        return next_obs, float(reward), done, info\n",
    "\n",
    "    def _settle_cost(self, net_grid_kw: float) -> float:\n",
    "        energy_kwh = net_grid_kw * self.delta_t\n",
    "        if energy_kwh >= 0:\n",
    "            return energy_kwh * self.tariff.import_price\n",
    "        return energy_kwh * self.tariff.export_price\n",
    "\n",
    "    def _pump_dispatch(self, action: int) -> Tuple[float, float]:\n",
    "        if action in (0, 4):\n",
    "            pumps = 0\n",
    "        elif action == 1:\n",
    "            pumps = 1\n",
    "        else:\n",
    "            pumps = self.cfg.num_pumps\n",
    "        pump_kw = pumps * self.cfg.pump_kw\n",
    "        energy_kwh = pump_kw * self.delta_t\n",
    "        irrig_m3 = energy_kwh * self.cfg.water_per_kw\n",
    "        return pump_kw, irrig_m3\n",
    "\n",
    "    def _update_soil(self, irrig_m3: float, irradiance: float) -> float:\n",
    "        if self.daily_target <= 0:\n",
    "            return 0.0\n",
    "        irrig_fraction = irrig_m3 / self.daily_target\n",
    "        self.remaining_irrigation = max(0.0, self.remaining_irrigation - irrig_fraction)\n",
    "        soil_gain = 0.8 * irrig_fraction\n",
    "        evap = 0.01 + 0.05 * irradiance\n",
    "        self.soil = float(np.clip(self.soil + soil_gain - evap, self.cfg.soil_min, self.cfg.soil_capacity))\n",
    "        return irrig_fraction\n",
    "\n",
    "    def _bess_dispatch(self, action: int, residual_kw: float) -> float:\n",
    "        if action == 3 and residual_kw > 0:\n",
    "            request = min(residual_kw, self.cfg.bess_kw)\n",
    "        elif action == 4 and residual_kw < 0:\n",
    "            request = max(residual_kw, -self.cfg.bess_kw)\n",
    "        else:\n",
    "            request = 0.0\n",
    "        return self._apply_bess(request)\n",
    "\n",
    "    def _apply_bess(self, requested_kw: float) -> float:\n",
    "        if requested_kw >= 0:\n",
    "            max_discharge_kw = min(self.cfg.bess_kw, self.soc / self.delta_t)\n",
    "            actual_kw = min(requested_kw, max_discharge_kw)\n",
    "            energy = actual_kw * self.delta_t\n",
    "            self.soc -= energy\n",
    "            return actual_kw\n",
    "        max_charge_kw = min(self.cfg.bess_kw, (self.cfg.bess_kwh - self.soc) / (self.cfg.bess_efficiency * self.delta_t))\n",
    "        charge_kw = min(-requested_kw, max_charge_kw)\n",
    "        energy = charge_kw * self.delta_t * self.cfg.bess_efficiency\n",
    "        self.soc += energy\n",
    "        return -charge_kw\n",
    "\n",
    "    def _record_step(self, action: int, pump_kw: float, bess_kw: float, grid_kw: float, pv_kw: float,\n",
    "                     base_kw: float, temp_norm: float, irrig_fraction: float, reward: float) -> None:\n",
    "        hour = float(self.day[\"hours\"][self.step_idx])\n",
    "        self._step_log.append({\n",
    "            \"hour\": hour,\n",
    "            \"action\": action,\n",
    "            \"pv_kw\": pv_kw,\n",
    "            \"base_kw\": base_kw,\n",
    "            \"pump_kw\": pump_kw,\n",
    "            \"bess_kw\": bess_kw,\n",
    "            \"grid_kw\": grid_kw,\n",
    "            \"soc\": self.soc / self.cfg.bess_kwh,\n",
    "            \"soil\": self.soil,\n",
    "            \"remaining_irrig\": self.remaining_irrigation,\n",
    "            \"temp_norm\": temp_norm,\n",
    "            \"irradiance\": float(self.day[\"irradiance\"][self.step_idx]),\n",
    "            \"irrig_frac\": irrig_fraction,\n",
    "            \"reward\": reward,\n",
    "        })\n",
    "\n",
    "    def _finalize_metrics(self) -> None:\n",
    "        imports = sum(max(entry[\"grid_kw\"], 0.0) for entry in self._step_log) * self.delta_t\n",
    "        exports = -sum(min(entry[\"grid_kw\"], 0.0) for entry in self._step_log) * self.delta_t\n",
    "        self.episode_metrics = {\n",
    "            \"total_cost\": self.cumulative_cost,\n",
    "            \"stress_hours\": self.cumulative_stress_steps * self.delta_t,\n",
    "            \"irrigation_shortfall\": self.remaining_irrigation,\n",
    "            \"bess_cycles\": self.bess_throughput / (2 * self.cfg.bess_kwh),\n",
    "            \"grid_import_kwh\": imports,\n",
    "            \"grid_export_kwh\": exports,\n",
    "            \"episode_reward\": self._episode_reward,\n",
    "        }\n",
    "        self.episode_trace = list(self._step_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-based policy for benchmarking\n",
    "def rule_based_policy(state: np.ndarray) -> int:\n",
    "    time_norm, soc, soil, pv_norm, base_norm, irr, temp, remaining = state.tolist()\n",
    "    if soil < 0.5 and pv_norm > 0.5:\n",
    "        return 2 if soc > 0.4 else 1\n",
    "    if remaining > 0.2 and soc > 0.6 and time_norm > 0.4:\n",
    "        return 3\n",
    "    if pv_norm > base_norm and soc < 0.9:\n",
    "        return 4\n",
    "    if soil < 0.45:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def random_policy(_: np.ndarray) -> int:\n",
    "    return random.randint(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight DQN components\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 20000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool) -> None:\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64,\n",
    "                 lr: float = 1e-3, gamma: float = 0.99, epsilon_start: float = 1.0,\n",
    "                 epsilon_end: float = 0.05, epsilon_decay: float = 0.995,\n",
    "                 batch_size: int = 128, target_update: int = 10, buffer_size: int = 25000):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update = target_update\n",
    "        self.replay = ReplayBuffer(buffer_size)\n",
    "        self.q_net = QNetwork(state_dim, action_dim, hidden_dim).to(DEVICE)\n",
    "        self.target_net = QNetwork(state_dim, action_dim, hidden_dim).to(DEVICE)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.learn_steps = 0\n",
    "\n",
    "    def select_action(self, state: np.ndarray, explore: bool = True) -> int:\n",
    "        if explore and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_tensor)\n",
    "        return int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done) -> None:\n",
    "        self.replay.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def learn(self) -> float:\n",
    "        if len(self.replay) < self.batch_size:\n",
    "            return 0.0\n",
    "        states, actions, rewards, next_states, dones = self.replay.sample(self.batch_size)\n",
    "        states_t = torch.from_numpy(states).float().to(DEVICE)\n",
    "        actions_t = torch.from_numpy(actions).long().to(DEVICE)\n",
    "        rewards_t = torch.from_numpy(rewards).float().to(DEVICE)\n",
    "        next_states_t = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "        dones_t = torch.from_numpy(dones.astype(np.float32)).to(DEVICE)\n",
    "\n",
    "        q_values = self.q_net(states_t).gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "        next_q = self.target_net(next_states_t).max(1)[0]\n",
    "        targets = rewards_t + self.gamma * (1 - dones_t) * next_q.detach()\n",
    "        loss = F.mse_loss(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.learn_steps += 1\n",
    "        return float(loss.item())\n",
    "\n",
    "    def update_target(self) -> None:\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def schedule_epsilon(self) -> None:\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        torch.save(self.q_net.state_dict(), path)\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        self.q_net.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "        self.update_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper\n",
    "def train_dqn(num_days: int = 220, seed_offset: int = 0) -> Tuple[DQNAgent, List[float], List[float]]:\n",
    "    env = AgrivoltaicEnv()\n",
    "    agent = DQNAgent(env.state_dim, env.action_space)\n",
    "    reward_history: List[float] = []\n",
    "    loss_history: List[float] = []\n",
    "    for episode in range(num_days):\n",
    "        state = env.reset(seed=seed_offset + episode)\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        ep_losses: List[float] = []\n",
    "        while not done:\n",
    "            action = agent.select_action(state, explore=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            loss = agent.learn()\n",
    "            if loss:\n",
    "                ep_losses.append(loss)\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "        agent.schedule_epsilon()\n",
    "        if (episode + 1) % agent.target_update == 0:\n",
    "            agent.update_target()\n",
    "        reward_history.append(ep_reward)\n",
    "        loss_history.append(float(np.mean(ep_losses)) if ep_losses else 0.0)\n",
    "    return agent, reward_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utilities and plotting\n",
    "def evaluate_policy(env: AgrivoltaicEnv, policy_fn: Callable[[np.ndarray], int], days: int = 20,\n",
    "                    seed_start: int = 1000) -> Tuple[List[Dict[str, float]], List[List[Dict[str, float]]]]:\n",
    "    metrics: List[Dict[str, float]] = []\n",
    "    traces: List[List[Dict[str, float]]] = []\n",
    "    for day in range(days):\n",
    "        state = env.reset(seed=seed_start + day)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy_fn(state)\n",
    "            state, _, done, _ = env.step(action)\n",
    "        metrics.append(env.episode_metrics)\n",
    "        traces.append(env.episode_trace)\n",
    "    return metrics, traces\n",
    "\n",
    "def summarize_metrics(metrics: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "    if not metrics:\n",
    "        return {}\n",
    "    summary: Dict[str, float] = {}\n",
    "    for key in metrics[0]:\n",
    "        values = np.array([m[key] for m in metrics], dtype=np.float32)\n",
    "        summary[key] = float(np.mean(values))\n",
    "    return summary\n",
    "\n",
    "def print_kpis(label: str, metrics: List[Dict[str, float]]) -> None:\n",
    "    summary = summarize_metrics(metrics)\n",
    "    if not summary:\n",
    "        print(f\"No metrics for {label}.\")\n",
    "        return\n",
    "    print(f\"\\n--- {label} KPIs ---\")\n",
    "    print(f\"Mean daily cost: ${summary['total_cost']:.2f}\")\n",
    "    print(f\"Stress hours: {summary['stress_hours']:.2f} h\")\n",
    "    print(f\"Irrigation shortfall: {summary['irrigation_shortfall']*100:.1f}% of target\")\n",
    "    print(f\"BESS cycles: {summary['bess_cycles']:.3f} equiv/day\")\n",
    "    print(f\"Import/Export: {summary['grid_import_kwh']:.1f} / {summary['grid_export_kwh']:.1f} kWh\")\n",
    "    print(f\"Episode reward: {summary['episode_reward']:.2f}\")\n",
    "\n",
    "def plot_training(rewards: List[float], losses: List[float]) -> None:\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "    ax1.plot(rewards, color=\"tab:blue\", label=\"Episode reward\")\n",
    "    ax1.set_xlabel(\"Episode\")\n",
    "    ax1.set_ylabel(\"Reward\", color=\"tab:blue\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(losses, color=\"tab:orange\", alpha=0.6, label=\"Loss\")\n",
    "    ax2.set_ylabel(\"Loss\", color=\"tab:orange\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.title(\"Training progress\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_day(trace: List[Dict[str, float]], title: str) -> None:\n",
    "    if not trace:\n",
    "        print(\"Empty trace\")\n",
    "        return\n",
    "    hours = [row[\"hour\"] for row in trace]\n",
    "    pv = [row[\"pv_kw\"] for row in trace]\n",
    "    base = [row[\"base_kw\"] for row in trace]\n",
    "    pump = [row[\"pump_kw\"] for row in trace]\n",
    "    bess = [row[\"bess_kw\"] for row in trace]\n",
    "    grid = [row[\"grid_kw\"] for row in trace]\n",
    "    soc = [row[\"soc\"] for row in trace]\n",
    "    soil = [row[\"soil\"] for row in trace]\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "    ax1.plot(hours, pv, label=\"PV kW\")\n",
    "    ax1.plot(hours, base, label=\"Base load kW\")\n",
    "    ax1.step(hours, pump, where=\"post\", label=\"Pump kW\")\n",
    "    ax1.plot(hours, grid, label=\"Grid kW\")\n",
    "    ax1.plot(hours, bess, label=\"BESS kW\")\n",
    "    ax1.set_ylabel(\"Power (kW)\")\n",
    "    ax1.set_xlabel(\"Hour\")\n",
    "    ax1.legend(loc=\"upper left\", ncol=2)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(hours, soc, color=\"tab:green\", label=\"SOC\")\n",
    "    ax2.plot(hours, soil, color=\"tab:brown\", label=\"Soil\")\n",
    "    ax2.set_ylabel(\"State (p.u.)\")\n",
    "    ax2.axhline(APV_CFG.soil_target, color=\"tab:brown\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.title(title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent and benchmark vs. rule-based policy\n",
    "agent, rewards, losses = train_dqn(num_days=220, seed_offset=0)\n",
    "plot_training(rewards, losses)\n",
    "\n",
    "eval_env = AgrivoltaicEnv()\n",
    "baseline_metrics, baseline_traces = evaluate_policy(eval_env, rule_based_policy, days=20, seed_start=2000)\n",
    "rl_policy = lambda state: agent.select_action(state, explore=False)\n",
    "rl_metrics, rl_traces = evaluate_policy(eval_env, rl_policy, days=20, seed_start=3000)\n",
    "\n",
    "print_kpis(\"Rule-based\", baseline_metrics)\n",
    "print_kpis(\"DQN\", rl_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize representative baseline vs RL day\n",
    "if baseline_traces:\n",
    "    plot_day(baseline_traces[0], \"Rule-based day\")\n",
    "if rl_traces:\n",
    "    plot_day(rl_traces[0], \"DQN day\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
